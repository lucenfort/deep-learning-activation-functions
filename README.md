# ğŸš€ FunÃ§Ãµes de AtivaÃ§Ã£o em Redes Neurais

## ğŸ“Œ IntroduÃ§Ã£o

As **funÃ§Ãµes de ativaÃ§Ã£o** sÃ£o componentes essenciais das redes neurais ğŸ§ , pois introduzem **nÃ£o linearidade** ao modelo, permitindo que a rede aprenda relaÃ§Ãµes complexas entre as entradas e saÃ­das.  

Sem essas funÃ§Ãµes, uma rede neural profunda seria **equivalente a uma simples combinaÃ§Ã£o linear** de suas camadas, tornando-a incapaz de resolver problemas mais complexos.  

ğŸ“– **Neste repositÃ³rio, exploramos diversas funÃ§Ãµes de ativaÃ§Ã£o**, abordando suas **representaÃ§Ãµes matemÃ¡ticas, aplicaÃ§Ãµes, vantagens e desvantagens**.

---

## ğŸ“– Teoria: Detalhamento das FunÃ§Ãµes de AtivaÃ§Ã£o

Aqui estÃ£o as principais funÃ§Ãµes de ativaÃ§Ã£o utilizadas em redes neurais, explicadas em detalhes:

### 1ï¸âƒ£ Sigmoid (Logistic Function) ğŸ“ˆ

- **FÃ³rmula:**  
  $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
- **Derivada:**  
  $$\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$$
- **Forma:** Curva em "S" (sigmoide).
- **AplicaÃ§Ãµes:**  
  âœ… Modelagem probabilÃ­stica  
  âœ… ClassificaÃ§Ã£o binÃ¡ria  
- **Problema:** Sofre com **gradiente desaparecendo**, dificultando o aprendizado em redes profundas.

---

### 2ï¸âƒ£ Tanh (Tangente HiperbÃ³lica) ğŸ”„

- **FÃ³rmula:**  
  $$tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
- **Derivada:**  
  $$tanh'(x) = 1 - tanh^2(x)$$
- **Forma:** Curva em "S", semelhante Ã  Sigmoid.
- **AplicaÃ§Ãµes:**  
  âœ… Melhor desempenho que Sigmoid em **camadas ocultas**, pois Ã© centrada em zero.  
- **Problema:** Ainda sofre de **gradiente desaparecendo**, mas menos que Sigmoid.

---

### 3ï¸âƒ£ Softmax ğŸ†

- **FÃ³rmula:**  
  $$S(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$$
- **Forma:** FunÃ§Ã£o vetorial que converte mÃºltiplas saÃ­das em probabilidades.
- **AplicaÃ§Ãµes:**  
  âœ… Usada na **camada de saÃ­da** para **classificaÃ§Ã£o multiclasse**.  
- **Problema:** Pode ser **sensÃ­vel a valores extremos**, entÃ£o recomenda-se normalizaÃ§Ã£o.

---

### 4ï¸âƒ£ ReLU âš¡ (Rectified Linear Unit)

- **FÃ³rmula:**  
  $$ReLU(x) = \max(0, x)$$
- **Derivada:**  
  $$ReLU'(x) = \begin{cases}

1, & x > 0 \\
0, & x \leq 0
\end{cases}$$

- **Forma:** Linear para valores positivos.
- **AplicaÃ§Ãµes:**  
  âœ… FunÃ§Ã£o mais utilizada em **redes profundas**.  
- **Problema:** NeurÃ´nios mortos (quando \( x < 0 \), a saÃ­da Ã© 0 para sempre).

---

### 5ï¸âƒ£ Leaky ReLU ğŸ’¡

- **FÃ³rmula:**  
  $$LeakyReLU(x) = \begin{cases}

x, & x > 0 \\
\alpha x, & x \leq 0
\end{cases}$$

- **AplicaÃ§Ãµes:**  
  âœ… Soluciona o problema de **neurÃ´nios mortos** da ReLU.

---

### 6ï¸âƒ£ ELU ğŸš€ (Exponential Linear Unit)

- **FÃ³rmula:**  
  $$ELU(x) = \begin{cases}

x, & x > 0 \\
\alpha (e^x - 1), & x \leq 0
\end{cases}$$

- **AplicaÃ§Ãµes:**  
  âœ… Propaga melhor o gradiente do que ReLU.

---

### 7ï¸âƒ£ Softsign ğŸ”„

- **FÃ³rmula:**  
  $$Softsign(x) = \frac{x}{1 + |x|}$$
- **AplicaÃ§Ãµes:**  
  âœ… Alternativa Ã  Tanh.

---

### 8ï¸âƒ£ Swish ğŸŒŸ

- **FÃ³rmula:**  
  $$Swish(x) = x \cdot \sigma(x)$$
- **AplicaÃ§Ãµes:**  
  âœ… Melhor desempenho em redes modernas como **EfficientNet**.

---

## ğŸ“Š GrÃ¡ficos das FunÃ§Ãµes e Suas Derivadas

![GrÃ¡fico das FunÃ§Ãµes de AtivaÃ§Ã£o e suas Derivadas](newplot.png)

ğŸ”¹ **FunÃ§Ã£o de ativaÃ§Ã£o** â†’ **Violeta** ğŸŸ£  
ğŸ”¸ **Derivada** â†’ **Laranja** ğŸŸ   

## ğŸ“œ ConclusÃ£o

ğŸ¯ As **funÃ§Ãµes de ativaÃ§Ã£o** desempenham um papel **crucial** no aprendizado profundo.  
ğŸ” Cada funÃ§Ã£o tem suas vantagens e desvantagens, e **a escolha certa pode impactar diretamente a performance da rede**.

âœ… **FunÃ§Ãµes como ReLU, ELU e Swish** sÃ£o as mais eficientes para redes neurais profundas.  
âŒ **Sigmoid e Tanh** sÃ£o mais usadas para camadas ocultas menores.  
ğŸ”¢ **Softmax** Ã© ideal para **classificaÃ§Ã£o multiclasse**.

> âš ï¸ **Nota sobre a Derivada da Softmax**  
>
> A **Softmax** Ã© uma **funÃ§Ã£o vetorial**, cuja derivada Ã© uma **matriz Jacobiana** ğŸ§®,  
> impossibilitando uma representaÃ§Ã£o grÃ¡fica direta.  
>
> ğŸ“Œ **SoluÃ§Ã£o:**  
> Para visualizaÃ§Ã£o, usamos a **derivada parcial de um Ãºnico neurÃ´nio** em relaÃ§Ã£o Ã  sua prÃ³pria entrada:  
>
> $$
> \frac{dSoftmax(x_i)}{dx_i} = Softmax(x_i) \cdot (1 - Softmax(x_i))
> $$
>
> ğŸ”¹ Esse termo corresponde Ã  derivada da **Sigmoid**, mas aplicada individualmente a cada saÃ­da da Softmax.

